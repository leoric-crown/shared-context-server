# Testing Guide

## Overview

This guide implements comprehensive testing patterns for the Shared Context MCP Server, focusing on behavioral testing, FastMCP in-memory testing, and multi-agent collaboration scenarios. The approach emphasizes testing what the system does rather than how it does it.

## Core Testing Philosophy

### Behavioral Testing Focus
- **Test what, not how**: Focus on observable behaviors and outcomes
- **User-centric scenarios**: Test from the agent's perspective
- **Implementation agnostic**: Tests should survive refactoring
- **Complete workflows**: Test end-to-end user journeys
- **Real-world scenarios**: Multi-agent collaboration patterns

### Test Pyramid Strategy
```
         /\
        /  \  E2E Tests (10%)
       /    \  - Critical user journeys
      /      \  - Multi-agent scenarios
     /--------\
    /          \  Integration Tests (30%)
   /            \  - MCP tool interactions
  /              \  - Database operations
 /                \  - API endpoints
/------------------\
                     Unit Tests (60%)
                     - Business logic
                     - Validation rules
                     - Utility functions
```

## Modern Database Testing Infrastructure

### Overview: Real Database Testing

**üéØ Migration Complete**: We've migrated from fragile hardcoded mock databases to modern real database testing infrastructure. This approach provides better test fidelity, handles schema evolution gracefully, and tests actual database constraints and behaviors.

### Key Benefits
- **Schema Evolution Support**: Tests remain valid when database schemas change
- **Real Database Behavior**: Tests actual database constraints, triggers, and SQLite-specific behavior
- **Better Test Fidelity**: No more gaps between mock behavior and real database behavior
- **Easier Maintenance**: No need to update multiple hardcoded mocks when schemas change
- **Isolation**: Each test gets a clean database state with proper test data seeding

### Test Configuration

```python
# conftest.py - Modern database testing fixtures
import inspect
import os
import tempfile
from contextlib import asynccontextmanager
from pathlib import Path
from typing import Any, AsyncGenerator

import pytest
from pydantic.fields import FieldInfo

from shared_context_server.database import DatabaseManager

@pytest.fixture(scope="function")
async def test_db_manager():
    """
    Create an isolated in-memory SQLite database manager for each test.

    This fixture provides a real database with the complete schema applied,
    ensuring tests work with actual database constraints and behaviors.
    Each test gets a clean database state.

    Yields:
        DatabaseManager: Initialized database manager with applied schema
    """
    # Create temporary database file for this test
    with tempfile.NamedTemporaryFile(suffix=".db", delete=False) as temp_db:
        temp_db_path = temp_db.name

    try:
        # Create database manager with temporary database
        db_manager = DatabaseManager(f"sqlite:///{temp_db_path}")

        # Initialize database with schema
        await db_manager.initialize()

        # Verify schema is correctly applied
        async with db_manager.get_connection() as conn:
            cursor = await conn.execute("SELECT MAX(version) FROM schema_version")
            version = await cursor.fetchone()
            assert version and version[0] == 2, f"Expected schema version 2, got {version[0] if version else None}"

        yield db_manager

    finally:
        # Clean up temporary database file
        if os.path.exists(temp_db_path):
            os.unlink(temp_db_path)

@pytest.fixture(scope="function")
async def test_db_connection(test_db_manager):
    """
    Provide a database connection for tests that need direct database access.

    Args:
        test_db_manager: The test database manager fixture

    Yields:
        aiosqlite.Connection: Database connection with optimized settings
    """
    async with test_db_manager.get_connection() as conn:
        yield conn
```

### FastMCP Integration with Real Database Testing

```python
# Integration pattern for FastMCP tools with real database
async def call_fastmcp_tool(fastmcp_tool, ctx, **kwargs):
    """
    Call a FastMCP tool function with proper default handling.

    This helper automatically extracts Field defaults and merges them
    with provided kwargs to avoid FieldInfo object issues.
    """
    # Get the actual defaults
    defaults = extract_field_defaults(fastmcp_tool)

    # Merge defaults with provided kwargs (kwargs override defaults)
    call_args = {**defaults, **kwargs}

    # Call the function with context as first parameter
    return await fastmcp_tool.fn(ctx, **call_args)

def patch_database_connection(test_db_manager):
    """
    Create a patcher for the global get_db_connection function.

    Args:
        test_db_manager: The test database manager to use

    Returns:
        unittest.mock.patch context manager
    """
    from unittest.mock import patch

    @asynccontextmanager
    async def mock_get_db_connection():
        async with test_db_manager.get_connection() as conn:
            yield conn

    return patch("shared_context_server.server.get_db_connection", mock_get_db_connection)
```

## Testing Patterns

### 1. Modern FastMCP Tool Testing

**‚úÖ Recommended Approach:**

```python
@pytest.mark.asyncio
async def test_message_creation_workflow(test_db_manager):
    """Test message creation using real database."""

    with patch("shared_context_server.server.get_db_connection") as mock_db_conn:
        # Use the real test database instead of hardcoded mocks
        @asynccontextmanager
        async def mock_get_db_connection():
            async with test_db_manager.get_connection() as conn:
                yield conn

        mock_db_conn.side_effect = mock_get_db_connection

        # Create test context
        ctx = MockContext("test_session", "test_agent")

        # Test session creation
        session_result = await call_fastmcp_tool(
            create_session,
            ctx,
            purpose="Test session",
            metadata={"test": True}
        )

        assert session_result["success"] is True
        session_id = session_result["session_id"]

        # Test message creation with proper schema support
        message_result = await call_fastmcp_tool(
            add_message,
            ctx,
            session_id=session_id,
            content="Test message",
            visibility="public",
            metadata={"agent": "test"}
        )

        assert message_result["success"] is True
```

**‚ùå Old Approach (Don't Use):**

```python
# DEPRECATED: Hardcoded mock database (fragile and unmaintainable)
@pytest.fixture
def mock_database():
    """DEPRECATED: Don't use hardcoded mocks."""
    messages = {}

    async def mock_execute(query, params=()):
        if "INSERT INTO messages" in query:
            # ‚ùå This breaks when schema changes!
            session_id, sender, content, visibility, metadata, parent_id = params  # Only 6 params!
            # Real schema now has 7 parameters including sender_type
            # This causes "Error binding parameter 7: type 'FieldInfo' is not supported"

    # ... hardcoded mock implementation
```

### 2. Migration Guide

If you encounter failing tests with messages like:
- `"Error binding parameter 7: type 'FieldInfo' is not supported"`
- Parameter count mismatches in mock databases

**Follow these steps:**

1. **Replace hardcoded mock fixtures** with `test_db_manager`
2. **Use `call_fastmcp_tool()` instead of `.fn()`** to handle FieldInfo defaults
3. **Patch database connections** properly using the new pattern
4. **Remove hardcoded parameter parsing** that expects specific counts

**Example Migration:**

```python
# Before (broken with schema changes)
@pytest.mark.asyncio
async def test_example(mock_database):
    with patch("server.get_db_connection") as mock_db:
        mock_db.return_value.__aenter__.return_value = mock_database
        result = await add_message.fn(ctx, session_id, content, visibility)  # ‚ùå FieldInfo error

# After (schema evolution safe)
@pytest.mark.asyncio
async def test_example(test_db_manager):
    with patch("server.get_db_connection") as mock_db:
        @asynccontextmanager
        async def mock_get_db_connection():
            async with test_db_manager.get_connection() as conn:
                yield conn
        mock_db.side_effect = mock_get_db_connection

        result = await call_fastmcp_tool(add_message, ctx, session_id=session_id, content=content)  # ‚úÖ Works
```

@pytest.fixture
async def test_database():
    """Create temporary test database with schema."""
    with tempfile.NamedTemporaryFile(suffix='.db', delete=False) as tmp:
        db_path = tmp.name

    # Initialize schema
    async with aiosqlite.connect(db_path) as conn:
        await conn.executescript("""
            CREATE TABLE sessions (
                id TEXT PRIMARY KEY,
                purpose TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                is_active BOOLEAN DEFAULT TRUE,
                created_by TEXT NOT NULL,
                metadata JSON
            );

            CREATE TABLE messages (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                session_id TEXT NOT NULL,
                sender TEXT NOT NULL,
                content TEXT NOT NULL,
                visibility TEXT DEFAULT 'public',
                message_type TEXT DEFAULT 'agent_response',
                metadata JSON,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                parent_message_id INTEGER,
                FOREIGN KEY (session_id) REFERENCES sessions(id),
                FOREIGN KEY (parent_message_id) REFERENCES messages(id)
            );

            CREATE TABLE agent_memory (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                agent_id TEXT NOT NULL,
                session_id TEXT,
                key TEXT NOT NULL,
                value TEXT NOT NULL,
                metadata JSON,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                expires_at TIMESTAMP,
                UNIQUE(agent_id, session_id, key)
            );

            -- Performance indexes
            CREATE INDEX idx_messages_session_time ON messages(session_id, timestamp);
            CREATE INDEX idx_messages_sender ON messages(sender, timestamp);
            CREATE INDEX idx_agent_memory_lookup ON agent_memory(agent_id, session_id, key);
        """)
        await conn.commit()

    yield db_path

    # Cleanup
    Path(db_path).unlink(missing_ok=True)

@pytest.fixture
async def mcp_server(test_database):
    """Create MCP server with test database."""
    from shared_context_server import create_mcp_server

    # Create server with test database
    server = await create_mcp_server(database_path=test_database)

    yield server

    # Cleanup server resources
    await server.cleanup()

@pytest.fixture
async def mcp_client(mcp_server):
    """Create FastMCP test client for in-memory testing."""
    async with TestClient(mcp_server) as client:
        yield client

@pytest.fixture
def agent_identities():
    """Test agent identities for multi-agent testing."""
    return {
        "claude_main": {
            "agent_id": "claude-main",
            "agent_type": "orchestrator",
            "permissions": ["create", "read", "write", "delete"]
        },
        "developer": {
            "agent_id": "developer-agent",
            "agent_type": "developer",
            "permissions": ["read", "write"]
        },
        "tester": {
            "agent_id": "tester-agent",
            "agent_type": "tester",
            "permissions": ["read", "write"]
        },
        "docs": {
            "agent_id": "docs-agent",
            "agent_type": "docs",
            "permissions": ["read"]
        }
    }
```

## Unit Testing Patterns

### Pure Function Testing
```python
def test_session_id_generation():
    """Test session ID generation behavior."""
    from shared_context_server.utils import generate_session_id

    # Test format
    session_id = generate_session_id()
    assert session_id.startswith("session_")
    assert len(session_id) == 24  # session_ + 16 hex chars

    # Test uniqueness
    ids = set(generate_session_id() for _ in range(100))
    assert len(ids) == 100  # All unique

def test_message_sanitization():
    """Test input sanitization behavior."""
    from shared_context_server.utils import sanitize_message_content

    # Test HTML escaping
    dirty_input = '<script>alert("xss")</script>Hello'
    clean_output = sanitize_message_content(dirty_input)
    assert '<script>' not in clean_output
    assert 'Hello' in clean_output

    # Test whitespace normalization
    messy_input = '  Multiple   spaces\n\nand\tlines  '
    clean_output = sanitize_message_content(messy_input)
    assert clean_output == 'Multiple spaces and lines'

def test_fuzzy_search_scoring():
    """Test fuzzy search scoring behavior."""
    from shared_context_server.search import calculate_fuzzy_score

    # Perfect match
    assert calculate_fuzzy_score("hello", "hello") == 100.0

    # No match
    assert calculate_fuzzy_score("hello", "xyz") == 0.0

    # Partial match
    score = calculate_fuzzy_score("hello world", "hello")
    assert 50.0 < score < 100.0
```

### Data Validation Testing
```python
@pytest.mark.asyncio
async def test_message_model_validation():
    """Test Pydantic message model validation."""
    from shared_context_server.models import MessageModel

    # Valid data should pass
    valid_data = {
        "session_id": "session_abc123",
        "sender": "test-agent",
        "content": "Test message",
        "visibility": "public"
    }
    model = MessageModel(**valid_data)
    assert model.session_id == "session_abc123"
    assert model.visibility == "public"

@pytest.mark.asyncio
async def test_message_model_validation_errors():
    """Test validation error behavior."""
    from shared_context_server.models import MessageModel
    from pydantic import ValidationError

    # Missing required field
    with pytest.raises(ValidationError, match="session_id.*required"):
        MessageModel(sender="test", content="hello")

    # Invalid visibility
    with pytest.raises(ValidationError, match="visibility.*invalid"):
        MessageModel(
            session_id="session_123",
            sender="test",
            content="hello",
            visibility="invalid"
        )
```

## FastMCP Integration Testing

‚ö†Ô∏è **CRITICAL: FastMCP Field Default Issue**

FastMCP tools decorated with `@mcp.tool()` have a critical issue when accessed via `.fn` attribute for testing. The Field default values become `FieldInfo` objects instead of actual default values, causing arithmetic and comparison errors.

### **‚úÖ WORKING SOLUTION: Inline Helper Functions**

The cleanest approach is to inline the helper functions directly in test files to avoid import complexity:

```python
# FastMCP testing helpers (inlined for simplicity)
import inspect
from pydantic.fields import FieldInfo

def extract_field_defaults(fastmcp_tool):
    """Extract actual default values from FastMCP tool function."""
    defaults = {}
    sig = inspect.signature(fastmcp_tool.fn)

    for param_name, param in sig.parameters.items():
        if param_name == 'ctx':  # Skip context parameter
            continue
        if isinstance(param.default, FieldInfo):
            defaults[param_name] = param.default.default
        elif param.default != inspect.Parameter.empty:
            defaults[param_name] = param.default
    return defaults

async def call_fastmcp_tool(fastmcp_tool, ctx, **kwargs):
    """Call FastMCP tool with proper default handling."""
    defaults = extract_field_defaults(fastmcp_tool)
    call_args = {**defaults, **kwargs}
    return await fastmcp_tool.fn(ctx, **call_args)

class MockContext:
    """Mock context for FastMCP testing."""
    def __init__(self, session_id="test_session", agent_id="test_agent"):
        self.session_id = session_id
        self.agent_id = agent_id
```

### **üéØ Key Testing Patterns Discovered**

1. **Agent ID Generation**: System uses consistent `getattr(ctx, "agent_id", f"agent_{ctx.session_id[:8]}")` pattern
   - MockContext("alice_session", "test_agent") ‚Üí sender becomes "test_agent" (uses explicit agent_id)
   - MockContext("alice_session") ‚Üí sender becomes "agent_alice_se" (fallback pattern)
   - Tests must use MockContext with explicit agent_id for predictable behavior

2. **Agent Memory Isolation Fix** ‚ö†Ô∏è **CRITICAL FIX APPLIED**
   - **Previous bug**: Memory system used session-based agent ID preventing isolation in shared sessions
   - **Root cause**: `agent_id = f"agent_{ctx.session_id[:8]}"` made agents in same session share memory
   - **Fix applied**: ALL server functions now use `getattr(ctx, 'agent_id', f"agent_{ctx.session_id[:8]}")`
   - **Functions updated**: `create_session`, `add_message`, `get_messages`, `get_session`, `search_context`
   - **Result**: Agents can share sessions while maintaining isolated memory
   - **Testing pattern**: Use same session_id but different agent_id values for isolation tests
   ```python
   # Correct pattern for testing agent isolation
   agent1_ctx = MockContext("shared_session", "agent_1")
   agent2_ctx = MockContext("shared_session", "agent_2")
   ```

3. **Database Row Factory Fix** ‚ö†Ô∏è **CRITICAL FIX APPLIED**
   - **Previous bug**: Database queries failed with "dictionary update sequence element #0 has length 24; 2 is required"
   - **Root cause**: Missing `conn.row_factory = aiosqlite.Row` in server functions
   - **Fix applied**: All server functions now set row factory for proper dict conversion
   - **Functions updated**: `get_session`, `get_messages`, `search_context`
   ```python
   async with get_db_connection() as conn:
       # Set row factory for dict-like access
       conn.row_factory = aiosqlite.Row
       # ... rest of database operations
   ```

4. **Visibility Filtering Logic** ‚ö†Ô∏è **MAJOR LOGIC OVERHAUL**
   - **Previous bug**: Visibility filters were additive instead of restrictive
   - **Root cause**: Adding visibility_filter as OR condition instead of replacing base rules
   - **Fix applied**: Complete rewrite of visibility filtering in `get_messages`
   - **New logic**:
     - With filter: Apply specific filter (`public`, `private`, `agent_only`) with agent access rules
     - Without filter: Default rules (public + own private/agent_only)
   ```python
   # New visibility filtering logic
   if visibility_filter:
       if visibility_filter == "public":
           where_conditions.append("visibility = 'public'")
       elif visibility_filter == "private":
           where_conditions.append("visibility = 'private' AND sender = ?")
           params.append(agent_id)
   else:
       # Default: public + own private/agent_only
       visibility_conditions = [
           "visibility = 'public'",
           "(visibility = 'private' AND sender = ?)",
           "(visibility = 'agent_only' AND sender = ?)",
       ]
       params.extend([agent_id, agent_id])
   ```

5. **Mock Database Parameter Updates** ‚ö†Ô∏è **PARAMETER FORMAT CHANGES**
   - **Issue**: Mock databases in behavioral tests expected old parameter formats
   - **Root cause**: Visibility filtering changes altered parameter structures
   - **Fix applied**: Updated mock database parameter parsing logic
   - **New parameter formats**:
     - Public filter: `(session_id, limit, offset)`
     - Private/agent_only: `(session_id, agent_id, limit, offset)`
     - No filter: `(session_id, agent_id, agent_id, limit, offset)`

6. **Database Mocking**: Use AsyncMock with proper return structure
   ```python
   mock_conn = AsyncMock()
   mock_conn.execute = AsyncMock(return_value=AsyncMock(lastrowid=None))
   mock_conn.commit = AsyncMock()
   ```

7. **Context Objects**: Must be objects with attributes, not dictionaries
   ```python
   # ‚úÖ Correct - object with attributes
   ctx = MockContext("session_123", "agent_456")

   # ‚ùå Wrong - dictionary
   ctx = {"session_id": "session_123", "agent_id": "agent_456"}
   ```

### Working FastMCP Testing Patterns

#### Method 1: Manual Parameter Passing (Explicit Approach)

```python
from tests.test_utils import MockContext

@pytest.mark.asyncio
async def test_create_session_tool_manual():
    """Test session creation using manual parameter passing."""
    from shared_context_server.server import create_session
    from unittest.mock import AsyncMock, patch

    # Create mock context (NOT a dict, but an object with attributes)
    ctx = MockContext("test_session_123", "test_agent")

    # Mock database operations
    mock_conn = AsyncMock()
    mock_conn.execute = AsyncMock(return_value=AsyncMock(lastrowid=None))
    mock_conn.commit = AsyncMock()

    with patch("shared_context_server.server.get_db_connection") as mock_db:
        mock_db.return_value.__aenter__.return_value = mock_conn

        # CRITICAL: Call .fn with context as first parameter
        # and explicit defaults to avoid FieldInfo objects
        result = await create_session.fn(
            ctx,  # Context as first parameter
            purpose="test session creation",
            metadata=None,  # Explicit default to avoid FieldInfo
        )

        # Verify response
        assert result["success"] is True
        assert "session_id" in result
        assert result["session_id"].startswith("session_")
        assert result["created_by"] == "agent_test_ses"  # Based on context.session_id

@pytest.mark.asyncio
async def test_set_memory_with_explicit_defaults():
    """Test memory setting with all explicit defaults."""
    from shared_context_server.server import set_memory
    from unittest.mock import AsyncMock, patch

    ctx = MockContext("test_session", "test_agent")

    mock_conn = AsyncMock()
    mock_conn.execute = AsyncMock(return_value=AsyncMock())
    mock_conn.commit = AsyncMock()

    with patch("shared_context_server.server.get_db_connection") as mock_db:
        mock_db.return_value.__aenter__.return_value = mock_conn

        # CRITICAL: Pass ALL parameters to avoid FieldInfo defaults
        result = await set_memory.fn(
            ctx,  # Context first
            key="test_key",
            value={"test": "data"},
            session_id="session_123",
            expires_in=None,      # Explicit default
            metadata=None,        # Explicit default
            overwrite=True,       # Explicit default
        )

        assert result["success"] is True
```

#### Method 2: Helper Function (Recommended Approach)

Create `tests/test_utils.py` with helper functions:

```python
"""Testing utilities for FastMCP-based shared context server."""

import inspect
from typing import Any, Dict
from pydantic.fields import FieldInfo

def extract_field_defaults(fastmcp_tool) -> Dict[str, Any]:
    """Extract actual default values from a FastMCP tool function."""
    defaults = {}
    sig = inspect.signature(fastmcp_tool.fn)

    for name, param in sig.parameters.items():
        if name == 'ctx':  # Skip context parameter
            continue

        if isinstance(param.default, FieldInfo):
            # Extract the actual default from FieldInfo
            defaults[name] = param.default.default
        elif param.default is not inspect.Parameter.empty:
            defaults[name] = param.default

    return defaults

async def call_fastmcp_tool(fastmcp_tool, ctx, **kwargs):
    """Call a FastMCP tool function with proper default handling."""
    # Get the actual defaults
    defaults = extract_field_defaults(fastmcp_tool)

    # Merge defaults with provided kwargs (kwargs override defaults)
    call_args = {**defaults, **kwargs}

    # Call the function with context as first parameter
    return await fastmcp_tool.fn(ctx, **call_args)

class MockContext:
    """Standard mock context for FastMCP testing."""

    def __init__(self, session_id="test_session", agent_id="test_agent"):
        self.session_id = session_id
        self.agent_id = agent_id
```

Using the helper function:

```python
@pytest.mark.asyncio
async def test_create_session_with_helper():
    """Test session creation using helper function."""
    from shared_context_server.server import create_session
    from tests.test_utils import call_fastmcp_tool, MockContext
    from unittest.mock import AsyncMock, patch

    ctx = MockContext("helper_session", "helper_agent")

    mock_conn = AsyncMock()
    mock_conn.execute = AsyncMock(return_value=AsyncMock(lastrowid=None))
    mock_conn.commit = AsyncMock()

    with patch("shared_context_server.server.get_db_connection") as mock_db:
        mock_db.return_value.__aenter__.return_value = mock_conn

        # Use helper - automatically handles defaults
        result = await call_fastmcp_tool(
            create_session,
            ctx,
            purpose="test session with helper"
            # metadata will use default (None) automatically
        )

        assert result["success"] is True
        assert result["session_id"].startswith("session_")

@pytest.mark.asyncio
async def test_search_context_with_helper():
    """Test context search using helper function."""
    from shared_context_server.server import search_context
    from tests.test_utils import call_fastmcp_tool, MockContext
    from unittest.mock import AsyncMock, patch

    ctx = MockContext("search_session", "search_agent")

    # Mock empty database
    mock_conn = AsyncMock()
    mock_conn.execute = AsyncMock(return_value=AsyncMock(fetchall=AsyncMock(return_value=[])))

    with patch("shared_context_server.server.get_db_connection") as mock_db:
        mock_db.return_value.__aenter__.return_value = mock_conn

        # Helper automatically applies defaults: fuzzy_threshold=60.0, limit=10, etc.
        result = await call_fastmcp_tool(
            search_context,
            ctx,
            session_id="session_123",
            query="test search query"
        )

        assert result["success"] is True
        assert "results" in result
        assert "search_time_ms" in result
```

#### Method 3: Debugging Field Defaults

To understand what defaults a FastMCP tool needs:

```python
from tests.test_utils import extract_field_defaults
from shared_context_server.server import set_memory, search_context

# See what defaults are needed
print("set_memory defaults:", extract_field_defaults(set_memory))
print("search_context defaults:", extract_field_defaults(search_context))

# Output shows:
# set_memory defaults: {'expires_in': None, 'metadata': None, 'overwrite': True, ...}
# search_context defaults: {'fuzzy_threshold': 60.0, 'limit': 10, 'search_metadata': True, ...}
```

### Real FastMCP Test Example

```python
@pytest.mark.asyncio
async def test_add_message_tool(ctx, mock_database):
    """Test message addition through MCP tool."""
    mcp_client.set_context(agent_identities["developer"])

    # Create session first
    session_result = await mcp_client.call_tool(
        "create_session",
        {"purpose": "message testing"}
    )
    session_id = session_result["session_id"]

    # Add message
    message_result = await mcp_client.call_tool(
        "add_message",
        {
            "session_id": session_id,
            "content": "Hello from developer agent",
            "visibility": "public",
            "metadata": {"type": "greeting"}
        }
    )

    # Verify message creation
    assert message_result["success"] is True
    assert "message_id" in message_result
    assert "timestamp" in message_result

@pytest.mark.asyncio
async def test_search_context_tool(mcp_client, agent_identities):
    """Test context search through MCP tool."""
    mcp_client.set_context(agent_identities["tester"])

    # Setup: Create session and add messages
    session_result = await mcp_client.call_tool("create_session", {"purpose": "search testing"})
    session_id = session_result["session_id"]

    messages = [
        "Implementing authentication middleware",
        "Writing unit tests for auth",
        "Documenting API endpoints",
        "Refactoring database queries"
    ]

    for msg in messages:
        await mcp_client.call_tool("add_message", {
            "session_id": session_id,
            "content": msg,
            "visibility": "public"
        })

    # Test fuzzy search
    search_result = await mcp_client.call_tool("search_context", {
        "session_id": session_id,
        "query": "authentication",
        "fuzzy_threshold": 60.0,
        "limit": 5
    })

    # Verify search results
    assert search_result["success"] is True
    assert len(search_result["results"]) > 0

    # Check that authentication-related message ranks highest
    top_result = search_result["results"][0]
    assert "authentication" in top_result["message"]["content"].lower()
    assert top_result["score"] > 60.0
```

### Resource Testing
```python
@pytest.mark.asyncio
async def test_session_resource_access(mcp_client, agent_identities):
    """Test session resource access."""
    mcp_client.set_context(agent_identities["claude_main"])

    # Create session with messages
    session_result = await mcp_client.call_tool("create_session", {"purpose": "resource test"})
    session_id = session_result["session_id"]

    await mcp_client.call_tool("add_message", {
        "session_id": session_id,
        "content": "First message",
        "visibility": "public"
    })

    # Read session resource
    resource = await mcp_client.read_resource(f"session://{session_id}")

    # Verify resource content
    assert resource.uri == f"session://{session_id}"
    assert resource.mimeType == "application/json"

    content = json.loads(resource.text)
    assert "session" in content
    assert "messages" in content
    assert len(content["messages"]) == 1
    assert content["messages"][0]["content"] == "First message"

@pytest.mark.asyncio
async def test_resource_subscription(mcp_client, agent_identities):
    """Test resource subscription for real-time updates."""
    mcp_client.set_context(agent_identities["developer"])

    # Create session
    session_result = await mcp_client.call_tool("create_session", {"purpose": "subscription test"})
    session_id = session_result["session_id"]

    # Subscribe to resource updates
    subscription = await mcp_client.subscribe_to_resource(f"session://{session_id}")
    assert subscription is not None

    # Add message (should trigger notification)
    await mcp_client.call_tool("add_message", {
        "session_id": session_id,
        "content": "New message should trigger update",
        "visibility": "public"
    })

    # Wait for notification
    notification = await mcp_client.wait_for_notification(timeout=5.0)
    assert notification is not None
    assert notification["uri"] == f"session://{session_id}"
```

## Multi-Agent Behavioral Testing

### Agent Collaboration Scenarios
```python
@pytest.mark.asyncio
async def test_multi_agent_collaboration(mcp_server, agent_identities):
    """Test multiple agents collaborating on shared context."""

    # Create separate clients for each agent
    async with TestClient(mcp_server) as claude_client, \
               TestClient(mcp_server) as dev_client, \
               TestClient(mcp_server) as test_client:

        # Set agent contexts
        claude_client.set_context(agent_identities["claude_main"])
        dev_client.set_context(agent_identities["developer"])
        test_client.set_context(agent_identities["tester"])

        # Claude creates session
        session_result = await claude_client.call_tool("create_session", {
            "purpose": "Multi-agent feature development"
        })
        session_id = session_result["session_id"]

        # Subscribe all agents to updates
        await dev_client.subscribe_to_resource(f"session://{session_id}")
        await test_client.subscribe_to_resource(f"session://{session_id}")

        # Claude adds initial requirements
        await claude_client.call_tool("add_message", {
            "session_id": session_id,
            "content": "Implement user authentication with JWT tokens",
            "metadata": {"type": "requirement", "priority": "high"}
        })

        # Developer responds with implementation plan
        await dev_client.call_tool("add_message", {
            "session_id": session_id,
            "content": "Plan: 1) Create JWT middleware 2) Add login endpoint 3) Secure existing routes",
            "metadata": {"type": "plan", "estimated_hours": 8}
        })

        # Tester adds testing considerations
        await test_client.call_tool("add_message", {
            "session_id": session_id,
            "content": "Testing approach: Unit tests for JWT validation, integration tests for auth flow",
            "metadata": {"type": "test_plan", "coverage_target": 0.9}
        })

        # Verify all messages are visible to all agents
        for client in [claude_client, dev_client, test_client]:
            resource = await client.read_resource(f"session://{session_id}")
            content = json.loads(resource.text)
            assert len(content["messages"]) == 3

            # Verify message types are preserved
            message_types = [msg["metadata"].get("type") for msg in content["messages"]]
            assert "requirement" in message_types
            assert "plan" in message_types
            assert "test_plan" in message_types

@pytest.mark.asyncio
async def test_private_agent_memory(mcp_server, agent_identities):
    """Test that agent memory is private and isolated."""

    async with TestClient(mcp_server) as dev_client, \
               TestClient(mcp_server) as test_client:

        dev_client.set_context(agent_identities["developer"])
        test_client.set_context(agent_identities["tester"])

        # Create shared session
        session_result = await dev_client.call_tool("create_session", {"purpose": "memory test"})
        session_id = session_result["session_id"]

        # Each agent stores private memory
        await dev_client.call_tool("set_memory", {
            "key": "current_task",
            "value": json.dumps({"task": "implement auth", "progress": 0.3}),
            "session_id": session_id
        })

        await test_client.call_tool("set_memory", {
            "key": "current_task",
            "value": json.dumps({"task": "write tests", "progress": 0.1}),
            "session_id": session_id
        })

        # Verify each agent can only access their own memory
        dev_memory = await dev_client.call_tool("get_memory", {
            "key": "current_task",
            "session_id": session_id
        })
        assert dev_memory["success"] is True
        task_data = json.loads(dev_memory["value"])
        assert task_data["task"] == "implement auth"

        test_memory = await test_client.call_tool("get_memory", {
            "key": "current_task",
            "session_id": session_id
        })
        assert test_memory["success"] is True
        task_data = json.loads(test_memory["value"])
        assert task_data["task"] == "write tests"

@pytest.mark.asyncio
async def test_message_visibility_controls(mcp_server, agent_identities):
    """Test message visibility controls work correctly."""

    async with TestClient(mcp_server) as dev_client, \
               TestClient(mcp_server) as test_client:

        dev_client.set_context(agent_identities["developer"])
        test_client.set_context(agent_identities["tester"])

        # Create session
        session_result = await dev_client.call_tool("create_session", {"purpose": "visibility test"})
        session_id = session_result["session_id"]

        # Developer adds public message
        await dev_client.call_tool("add_message", {
            "session_id": session_id,
            "content": "Public implementation notes",
            "visibility": "public"
        })

        # Developer adds private message
        await dev_client.call_tool("add_message", {
            "session_id": session_id,
            "content": "Private debugging thoughts",
            "visibility": "private"
        })

        # Tester adds public message
        await test_client.call_tool("add_message", {
            "session_id": session_id,
            "content": "Test results look good",
            "visibility": "public"
        })

        # Developer should see all their own messages (public + private)
        dev_resource = await dev_client.read_resource(f"session://{session_id}")
        dev_content = json.loads(dev_resource.text)
        dev_messages = [msg["content"] for msg in dev_content["messages"]]

        # Should see public messages from both agents, plus their own private
        assert "Public implementation notes" in dev_messages
        assert "Private debugging thoughts" in dev_messages  # Own private message
        assert "Test results look good" in dev_messages

        # Tester should only see public messages
        test_resource = await test_client.read_resource(f"session://{session_id}")
        test_content = json.loads(test_resource.text)
        test_messages = [msg["content"] for msg in test_content["messages"]]

        # Should see public messages from both agents, but not developer's private
        assert "Public implementation notes" in test_messages
        assert "Private debugging thoughts" not in test_messages  # Not visible
        assert "Test results look good" in test_messages
```

## End-to-End Testing

### Complete Workflow Testing
```python
@pytest.mark.asyncio
async def test_complete_feature_development_workflow(mcp_server, agent_identities):
    """Test complete multi-agent feature development workflow."""

    async with TestClient(mcp_server) as claude_client, \
               TestClient(mcp_server) as dev_client, \
               TestClient(mcp_server) as test_client, \
               TestClient(mcp_server) as docs_client:

        # Set contexts
        claude_client.set_context(agent_identities["claude_main"])
        dev_client.set_context(agent_identities["developer"])
        test_client.set_context(agent_identities["tester"])
        docs_client.set_context(agent_identities["docs"])

        # 1. Claude creates session and defines requirements
        session_result = await claude_client.call_tool("create_session", {
            "purpose": "Implement password reset functionality",
            "metadata": {"epic": "user-auth", "sprint": "2024-Q1"}
        })
        session_id = session_result["session_id"]

        await claude_client.call_tool("add_message", {
            "session_id": session_id,
            "content": "Requirements: Password reset via email with secure token expiration",
            "metadata": {"type": "requirement"}
        })

        # 2. Developer breaks down into tasks
        await dev_client.call_tool("add_message", {
            "session_id": session_id,
            "content": "Implementation tasks: 1) Email service integration 2) Token generation/validation 3) Reset endpoint 4) Frontend form",
            "metadata": {"type": "breakdown"}
        })

        # 3. Tester defines acceptance criteria
        await test_client.call_tool("add_message", {
            "session_id": session_id,
            "content": "Test scenarios: Valid email sends token, invalid email fails gracefully, expired tokens rejected, successful reset updates password",
            "metadata": {"type": "acceptance_criteria"}
        })

        # 4. Developer implements (simulated with memory)
        await dev_client.call_tool("set_memory", {
            "key": "implementation_status",
            "value": json.dumps({
                "email_service": "completed",
                "token_generation": "completed",
                "reset_endpoint": "in_progress",
                "frontend_form": "pending"
            }),
            "session_id": session_id
        })

        await dev_client.call_tool("add_message", {
            "session_id": session_id,
            "content": "Progress update: Email service and tokens implemented, working on reset endpoint",
            "metadata": {"type": "progress", "completion": 0.5}
        })

        # 5. Tester runs initial tests
        await test_client.call_tool("add_message", {
            "session_id": session_id,
            "content": "Unit tests passing for email service and token generation. Integration tests pending endpoint completion.",
            "metadata": {"type": "test_results", "status": "partial_pass"}
        })

        # 6. Developer completes implementation
        await dev_client.call_tool("set_memory", {
            "key": "implementation_status",
            "value": json.dumps({
                "email_service": "completed",
                "token_generation": "completed",
                "reset_endpoint": "completed",
                "frontend_form": "completed"
            }),
            "session_id": session_id
        })

        await dev_client.call_tool("add_message", {
            "session_id": session_id,
            "content": "Feature complete: All password reset functionality implemented and tested locally",
            "metadata": {"type": "completion", "files_modified": ["auth.py", "email.py", "reset.html"]}
        })

        # 7. Tester runs full test suite
        await test_client.call_tool("add_message", {
            "session_id": session_id,
            "content": "All tests passing: 15 unit tests, 8 integration tests, 4 E2E scenarios. Feature ready for deployment.",
            "metadata": {"type": "test_complete", "coverage": 0.95}
        })

        # 8. Docs agent adds documentation
        await docs_client.call_tool("add_message", {
            "session_id": session_id,
            "content": "Documentation updated: API endpoints documented, user guide updated with password reset flow",
            "metadata": {"type": "documentation", "pages_updated": ["api-auth.md", "user-guide.md"]}
        })

        # 9. Claude searches context for final summary
        search_result = await claude_client.call_tool("search_context", {
            "session_id": session_id,
            "query": "completion status tests documentation",
            "fuzzy_threshold": 50.0,
            "limit": 10
        })

        # Verify complete workflow captured
        resource = await claude_client.read_resource(f"session://{session_id}")
        content = json.loads(resource.text)

        # Should have messages from all agent types
        message_types = set()
        for msg in content["messages"]:
            if "type" in msg.get("metadata", {}):
                message_types.add(msg["metadata"]["type"])

        expected_types = {"requirement", "breakdown", "acceptance_criteria", "progress", "completion", "test_complete", "documentation"}
        assert expected_types.issubset(message_types)

        # Search should find relevant completion messages
        assert search_result["success"] is True
        assert len(search_result["results"]) > 0
```

## Performance Testing

### Load Testing
```python
@pytest.mark.benchmark
@pytest.mark.asyncio
async def test_concurrent_agent_performance(mcp_server, agent_identities):
    """Test system performance with multiple concurrent agents."""
    import time

    async def agent_workload(agent_id: str, session_id: str):
        """Simulate typical agent workload."""
        async with TestClient(mcp_server) as client:
            client.set_context({"agent_id": agent_id})

            # Each agent adds 10 messages
            for i in range(10):
                await client.call_tool("add_message", {
                    "session_id": session_id,
                    "content": f"Message {i} from {agent_id}",
                    "metadata": {"iteration": i}
                })

            # Each agent performs 5 searches
            for i in range(5):
                await client.call_tool("search_context", {
                    "session_id": session_id,
                    "query": f"message from {agent_id}",
                    "limit": 5
                })

            return f"{agent_id}_completed"

    # Create session
    async with TestClient(mcp_server) as setup_client:
        setup_client.set_context(agent_identities["claude_main"])
        session_result = await setup_client.call_tool("create_session", {
            "purpose": "Performance test session"
        })
        session_id = session_result["session_id"]

    # Run 20 agents concurrently
    agents = [f"agent_{i}" for i in range(20)]

    start_time = time.perf_counter()

    tasks = [agent_workload(agent_id, session_id) for agent_id in agents]
    results = await asyncio.gather(*tasks)

    end_time = time.perf_counter()
    elapsed_ms = (end_time - start_time) * 1000

    # Verify all agents completed successfully
    assert len(results) == 20
    assert all("completed" in result for result in results)

    # Performance targets
    assert elapsed_ms < 10000, f"20 concurrent agents took {elapsed_ms:.2f}ms (should be < 10s)"

    # Verify final state
    async with TestClient(mcp_server) as verify_client:
        resource = await verify_client.read_resource(f"session://{session_id}")
        content = json.loads(resource.text)

        # Should have 200 total messages (20 agents √ó 10 messages each)
        assert len(content["messages"]) == 200

@pytest.mark.benchmark
@pytest.mark.asyncio
async def test_search_performance_large_dataset(mcp_server, agent_identities):
    """Test search performance with large message dataset."""
    import time

    async with TestClient(mcp_server) as client:
        client.set_context(agent_identities["claude_main"])

        # Create session
        session_result = await client.call_tool("create_session", {
            "purpose": "Large dataset search test"
        })
        session_id = session_result["session_id"]

        # Add 1000 messages with varied content
        messages = [
            f"Implementation of feature {i} with authentication middleware and database integration"
            for i in range(1000)
        ]

        # Add messages in batches for efficiency
        batch_size = 50
        for i in range(0, len(messages), batch_size):
            batch_tasks = []
            for j in range(i, min(i + batch_size, len(messages))):
                task = client.call_tool("add_message", {
                    "session_id": session_id,
                    "content": messages[j],
                    "metadata": {"index": j}
                })
                batch_tasks.append(task)
            await asyncio.gather(*batch_tasks)

        # Test search performance
        queries = [
            "authentication middleware implementation",
            "database integration feature",
            "implementation of feature 500",
            "middleware and database"
        ]

        search_times = []
        for query in queries:
            start_time = time.perf_counter()

            result = await client.call_tool("search_context", {
                "session_id": session_id,
                "query": query,
                "fuzzy_threshold": 60.0,
                "limit": 10
            })

            end_time = time.perf_counter()
            search_time_ms = (end_time - start_time) * 1000
            search_times.append(search_time_ms)

            # Verify search returns relevant results
            assert result["success"] is True
            assert len(result["results"]) > 0

            # Verify results are ranked by relevance
            scores = [r["score"] for r in result["results"]]
            assert scores == sorted(scores, reverse=True)

        # Performance target: search should complete in under 100ms
        avg_search_time = sum(search_times) / len(search_times)
        max_search_time = max(search_times)

        assert avg_search_time < 100, f"Average search time {avg_search_time:.2f}ms exceeds 100ms target"
        assert max_search_time < 200, f"Max search time {max_search_time:.2f}ms exceeds 200ms threshold"
```

## Test Organization

### File Structure
```
tests/
‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py              # Pydantic model validation
‚îÇ   ‚îú‚îÄ‚îÄ test_utils.py               # Utility functions
‚îÇ   ‚îú‚îÄ‚îÄ test_search.py              # Search algorithms
‚îÇ   ‚îî‚îÄ‚îÄ test_sanitization.py       # Input sanitization
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îú‚îÄ‚îÄ test_mcp_tools.py           # MCP tool functionality
‚îÇ   ‚îú‚îÄ‚îÄ test_resources.py          # MCP resource access
‚îÇ   ‚îú‚îÄ‚îÄ test_database.py           # Database operations
‚îÇ   ‚îî‚îÄ‚îÄ test_authentication.py     # Auth and permissions
‚îú‚îÄ‚îÄ behavioral/
‚îÇ   ‚îú‚îÄ‚îÄ test_multi_agent.py        # Multi-agent collaboration
‚îÇ   ‚îú‚îÄ‚îÄ test_workflows.py          # Complete workflows
‚îÇ   ‚îú‚îÄ‚îÄ test_privacy.py            # Privacy and visibility
‚îÇ   ‚îî‚îÄ‚îÄ test_real_time.py          # Subscriptions and updates
‚îú‚îÄ‚îÄ performance/
‚îÇ   ‚îú‚îÄ‚îÄ test_load.py                # Concurrent usage
‚îÇ   ‚îú‚îÄ‚îÄ test_search_speed.py       # Search performance
‚îÇ   ‚îî‚îÄ‚îÄ test_memory_usage.py       # Memory efficiency
‚îî‚îÄ‚îÄ conftest.py                     # Shared fixtures
```

## Best Practices

### Test Naming Conventions
```python
# Good test names describe the behavior being tested
def test_agent_can_create_session_with_valid_purpose():
    """Test that agents can create sessions when providing valid purpose."""
    pass

def test_private_message_only_visible_to_sender():
    """Test that private messages are only visible to the sending agent."""
    pass

def test_fuzzy_search_returns_ranked_results():
    """Test that fuzzy search returns results ranked by relevance score."""
    pass
```

### Test Documentation
```python
@pytest.mark.asyncio
async def test_multi_agent_subscription_workflow():
    """
    Test complete multi-agent subscription workflow.

    Scenario: Three agents collaborate on shared context with real-time updates
    1. Agent A creates session and subscribes to updates
    2. Agent B joins session and adds message
    3. Agent A receives notification of new message
    4. Agent C adds private message (should not notify others)
    5. Agents A and B should see public messages, not private

    Verifies:
    - Real-time subscription notifications work
    - Privacy controls are respected in notifications
    - Multiple concurrent subscriptions are supported
    """
    pass
```

### Fixture Management
```python
# Use appropriate fixture scopes
@pytest.fixture(scope="session")  # Expensive setup, reuse across all tests
def event_loop():
    pass

@pytest.fixture(scope="function")  # Fresh database for each test
async def test_database():
    pass

# Parametrized fixtures for testing multiple scenarios
@pytest.fixture(params=["public", "private", "agent_only"])
def message_visibility(request):
    return request.param
```

## Continuous Integration

### Test Running Strategy ‚ö†Ô∏è **PHASE-SPECIFIC QUALITY GATES**

**Mandatory Quality Commands** ‚ö†Ô∏è **MUST PASS BEFORE EACH PHASE**:
```bash
# Phase 3 quality gates (70% coverage minimum)
uv run ruff check .                                          # MANDATORY: Zero lint errors
uv run mypy .                                               # MANDATORY: Zero type errors
uv run pytest tests/ --cov=src --cov-fail-under=70         # MANDATORY: 70%+ coverage

# Phase 4 quality gates (85% coverage minimum + performance)
uv run ruff check .                                          # MANDATORY: Zero lint errors
uv run mypy .                                               # MANDATORY: Zero type errors
uv run pytest tests/ --cov=src --cov-fail-under=85         # MANDATORY: 85%+ coverage
uv run pytest tests/performance/ -v --benchmark-only        # MANDATORY: Performance targets met
```

```yaml
# GitHub Actions example
test_strategy:
  quality_gates:
    command: |
      uv run ruff check . &&
      uv run mypy . &&
      uv run pytest tests/ --cov=src --cov-fail-under=${COVERAGE_TARGET}
    timeout: 10 minutes

  unit_tests:
    command: pytest tests/unit/ -v --tb=short
    timeout: 5 minutes

  integration_tests:
    command: pytest tests/integration/ -v --tb=short
    timeout: 15 minutes

  behavioral_tests:
    command: pytest tests/behavioral/ -v --tb=short
    timeout: 20 minutes

  performance_tests:
    command: pytest tests/performance/ -v --tb=short -m benchmark
    timeout: 30 minutes
    required_for: phase_4_only
```

### Coverage Requirements ‚ö†Ô∏è **PHASE-SPECIFIC COVERAGE TARGETS**

**Progressive Coverage Targets**:
- **Phase 1-2 (Completed)**: 54% baseline coverage achieved
- **Phase 3 (Multi-Agent)**: ‚â•70% coverage REQUIRED before implementation
- **Phase 4 (Production)**: ‚â•85% coverage REQUIRED before implementation

**Module-Specific Coverage Expectations**:
- `database.py`: 80% (excellent) ‚Üí maintain 85%+
- `server.py`: 56% ‚Üí 75%+ (Phase 3), 85%+ (Phase 4)
- `models.py`: 59% ‚Üí 80%+ (Phase 3), 85%+ (Phase 4)
- New modules: Must achieve 90%+ during development

**Quality Gate Commands**:
```bash
# Phase 3 coverage validation (70% minimum)
uv run pytest tests/ --cov=src --cov-report=html --cov-fail-under=70

# Phase 4 coverage validation (85% minimum)
uv run pytest tests/ --cov=src --cov-report=html --cov-fail-under=85

# Performance testing (Phase 4)
uv run pytest tests/performance/ -v --benchmark-only
```

```python
# pytest.ini
[tool:pytest]
addopts = --cov=shared_context_server --cov-report=html --cov-report=term
testpaths = tests
markers =
    benchmark: performance and load testing
    integration: integration tests requiring database
    behavioral: multi-agent behavioral tests

# Coverage configuration
[tool.coverage.run]
source = ["src"]
omit = [
    "*/tests/*",
    "*/migrations/*",
    "*/venv/*"
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError"
]
# Phase-specific fail_under targets
fail_under = 70  # Phase 3 minimum, update to 85 for Phase 4
```

## Success Criteria

### Test Quality Success ‚ö†Ô∏è **PHASE-SPECIFIC QUALITY STANDARDS**
- **Tests focus on behavior, not implementation details**
- **All critical user workflows have test coverage**
- **Multi-agent scenarios are thoroughly tested**
- **Edge cases and error conditions are covered**
- **Tests are fast, reliable, and maintainable**
- **Phase-specific coverage targets achieved**: 70% (Phase 3), 85% (Phase 4)

### Coverage Success ‚ö†Ô∏è **MANDATORY QUALITY GATES**
- **Phase 3**: ‚â•70% coverage of critical business logic REQUIRED
- **Phase 4**: ‚â•85% coverage of critical business logic REQUIRED
- **All MCP tools and resources have comprehensive tests**
- **Error handling paths are tested**
- **Performance requirements are validated** (Phase 4: <100ms API, 20+ agents)
- **Multi-agent collaboration patterns are verified**
- **Quality gate enforcement**: Implementation MUST NOT proceed without passing coverage targets

### Test Maintainability Success
- Tests survive refactoring without modification
- Test failures indicate actual problems, not false positives
- New tests follow established patterns and conventions
- Test code is clean and well-organized
- Tests provide clear failure messages

## Testing Standardization Findings

‚ö†Ô∏è **COMPREHENSIVE TEST STANDARDIZATION COMPLETED**

During the comprehensive test standardization effort, critical implementation fixes were discovered and applied across the entire codebase. This resulted in achieving **97/97 tests passing (100% success rate)** across all test categories.

### Critical Implementation Fixes Applied

#### 1. Agent ID Consistency Fix ‚ö†Ô∏è **SYSTEM-WIDE CRITICAL FIX**
- **Issue**: Agent ID generation was inconsistent across server functions
- **Root Cause**: Some functions used old `agent_id = f"agent_{ctx.session_id[:8]}"` while others used new `getattr(ctx, "agent_id", f"agent_{ctx.session_id[:8]}")` pattern
- **Impact**: Tests expecting specific agent ID formats failed unpredictably
- **Fix Applied**: Updated ALL server functions to use consistent pattern:
  ```python
  agent_id = getattr(ctx, "agent_id", f"agent_{ctx.session_id[:8]}")
  ```
- **Functions Updated**: `create_session`, `add_message`, `get_messages`, `get_session`, `search_context`
- **Result**: Predictable agent ID generation, proper agent memory isolation in shared sessions

#### 2. Database Row Factory Fix ‚ö†Ô∏è **DATABASE OPERATION CRITICAL FIX**
- **Issue**: Database queries failed with "dictionary update sequence element #0 has length 24; 2 is required"
- **Root Cause**: Missing `conn.row_factory = aiosqlite.Row` in server functions
- **Impact**: Database operations returned tuples instead of dict-like rows
- **Fix Applied**: Added row factory to ALL database operations:
  ```python
  async with get_db_connection() as conn:
      conn.row_factory = aiosqlite.Row  # Enable dict-like access
      # ... rest of database operations
  ```
- **Functions Updated**: `get_session`, `get_messages`, `search_context`
- **Result**: Proper dict-like database row access throughout system

#### 3. Visibility Filtering Logic Overhaul ‚ö†Ô∏è **MAJOR SECURITY LOGIC FIX**
- **Issue**: Visibility filters were additive instead of restrictive, breaking security boundaries
- **Root Cause**: Adding visibility_filter as OR condition instead of replacing base visibility rules
- **Impact**: Agents could potentially access messages they shouldn't see
- **Fix Applied**: Complete rewrite of visibility filtering in `get_messages`:
  ```python
  # New restrictive visibility logic
  if visibility_filter:
      if visibility_filter == "public":
          where_conditions.append("visibility = 'public'")
      elif visibility_filter == "private":
          where_conditions.append("visibility = 'private' AND sender = ?")
          params.append(agent_id)
      elif visibility_filter == "agent_only":
          where_conditions.append("visibility = 'agent_only' AND sender = ?")
          params.append(agent_id)
  else:
      # Default visibility rules: public + own private/agent_only
      visibility_conditions = [
          "visibility = 'public'",
          "(visibility = 'private' AND sender = ?)",
          "(visibility = 'agent_only' AND sender = ?)",
      ]
      params.extend([agent_id, agent_id])
      where_conditions.append(f"({' OR '.join(visibility_conditions)})")
  ```
- **Result**: Proper security boundaries enforced, visibility filters work as expected

#### 4. Mock Database Parameter Format Updates ‚ö†Ô∏è **TEST INFRASTRUCTURE FIX**
- **Issue**: Mock databases in behavioral tests expected old parameter formats
- **Root Cause**: Visibility filtering changes altered SQL parameter structures
- **Impact**: Behavioral tests failed with parameter format mismatches
- **Fix Applied**: Updated mock database parameter parsing logic for new formats:
  - Public filter: `(session_id, limit, offset)`
  - Private/agent_only: `(session_id, agent_id, limit, offset)`
  - No filter: `(session_id, agent_id, agent_id, limit, offset)`
- **Files Updated**: `tests/behavioral/test_visibility.py`, `tests/integration/test_agent_workflow.py`
- **Result**: Mock databases properly handle new parameter formats

#### 5. Database Test Infrastructure Fixes ‚ö†Ô∏è **DATABASE TESTING CRITICAL FIXES**
- **Multiple Issues**:
  - Index name mismatch: `idx_messages_session_timestamp` ‚Üí `idx_messages_session_time`
  - Session ID format validation issues
  - Global function database consistency problems
  - Trigger timing issues requiring longer sleep periods
- **Fix Applied**: Comprehensive database test infrastructure improvements:
  ```python
  # Fixed global function consistency
  with patch.dict(os.environ, {"DATABASE_PATH": str(test_db_manager.database_path)}):
      import src.shared_context_server.database as db_module
      db_module._db_manager = test_db_manager
      stats = await cleanup_expired_data()
  ```
- **Result**: All 33/33 database tests passing

#### 6. Test Pattern Standardization ‚ö†Ô∏è **TESTING INFRASTRUCTURE STANDARDIZATION**
- **Issue**: Inconsistent test patterns across unit, integration, and behavioral tests
- **Root Cause**: Mix of obsolete `mcp.context` patches and modern direct context passing
- **Fix Applied**: Standardized ALL test files to use consistent `call_fastmcp_tool` pattern:
  ```python
  # Standardized test pattern
  result = await call_fastmcp_tool(
      fastmcp_tool_function,
      mock_context,
      **test_parameters
  )
  ```
- **Files Standardized**: All unit, integration, behavioral, and end-to-end test files
- **Result**: Consistent, maintainable test patterns across entire codebase

### Test Success Rate Achievement

**Final Test Results: 97/97 tests passing (100% success rate)**

#### By Test Category:
- **Database Tests**: 33/33 passing (100%)
- **Behavioral Tests**: 6/6 passing (100%)
- **Integration Tests**: All passing (100%)
- **Unit Tests**: All passing (100%)
- **Phase 1 End-to-End Tests**: 4/4 passing (100%)

#### Key Test Categories Fixed:
1. **Agent Memory Isolation**: Tests now properly validate agent memory isolation in shared sessions
2. **Visibility Controls**: All visibility filtering tests pass with proper security boundaries
3. **Database Operations**: Schema, triggers, constraints, and performance tests all pass
4. **FastMCP Integration**: All tools work consistently with standardized test patterns
5. **Multi-Agent Workflows**: Complex agent interaction scenarios work reliably

### FastMCP Testing Patterns - Standardized Approach

**Final Recommended Pattern**: Use helper functions with consistent MockContext objects:

```python
# Standard test pattern (now used across all test files)
from conftest import call_fastmcp_tool, MockContext

@pytest.mark.asyncio
async def test_example_functionality():
    """Test description."""
    ctx = MockContext("test_session_id", "test_agent_id")

    with patch("shared_context_server.server.get_db_connection") as mock_db:
        # Setup mocks...

        result = await call_fastmcp_tool(
            fastmcp_tool_function,
            ctx,
            parameter1="value1",
            parameter2="value2"
        )

        assert result["success"] is True
```

### Implementation Quality Standards Achieved

1. **Zero Regression Tolerance**: All existing functionality preserved while fixing critical bugs
2. **Security-First Approach**: Visibility filtering logic completely rewritten to ensure proper security boundaries
3. **Consistency Standards**: Agent ID generation, database operations, and test patterns now consistent system-wide
4. **Performance Maintained**: All fixes applied with no performance degradation
5. **Test Coverage**: 100% test success rate maintained throughout all fixes

## References

- FastMCP Testing: https://github.com/jlowin/fastmcp/docs/testing
- Pytest Documentation: https://docs.pytest.org/
- Behavioral Testing: https://martinfowler.com/articles/practical-test-pyramid.html
- AsyncIO Testing: https://docs.python.org/3/library/asyncio-dev.html#testing

## Related Guides

- Core Architecture Guide - System design and components
- Framework Integration Guide - MCP server implementation
- Performance Optimization Guide - Performance patterns and monitoring
